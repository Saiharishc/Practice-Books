{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NLTK is the most common package you will encounter working with corpora, \n",
    "categorizing text, analyzing linguistic structure...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'linguistic',\n",
       " 'unit',\n",
       " 'that',\n",
       " 'is',\n",
       " 'larger',\n",
       " 'than',\n",
       " 'a',\n",
       " 'single',\n",
       " 'sentence']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_ = \"A linguistic unit that is larger than a single sentence\"\n",
    "tokens_ = nltk.word_tokenize(sent_)\n",
    "tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting synonym of a word\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('spectacular.n.01'), Synset('dramatic.s.02'), Synset('spectacular.s.02'), Synset('outstanding.s.02')]\n"
     ]
    }
   ],
   "source": [
    "word_ = wordnet.synsets('spectacular')\n",
    "print(word_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a lavishly produced performance\n",
      "sensational in appearance or thrilling in effect\n",
      "characteristic of spectacles or drama\n",
      "having a quality that thrusts itself into attention\n"
     ]
    }
   ],
   "source": [
    "print(word_[0].definition())\n",
    "print(word_[1].definition())\n",
    "print(word_[2].definition())\n",
    "print(word_[3].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Stemming and Lemmatizing words :***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word stemming means removing affixes from words and returning the root word\n",
    "(which may not be real word).\n",
    "Lemmatizing is similar to stemming , but the difference is that the result \n",
    "of lemmatizing is a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()  #Create the stemmer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decreas\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('decreases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decrease\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()  #Create Lemmatizer object\n",
    "print(lemmatizer.lemmatize('decreases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TextBlob :***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is a python library for processing textual data.\n",
    "It provides a simple API for diving deep into common NLP tasks,\n",
    "such as part-of speech tagging, noun, phrase extraction, sentimental analysis,\n",
    "classification and more. \n",
    "We can use this for sentimental analysis. Sentiment refers to feeling hidden in the sentence.\n",
    "Polarity defines negativity or positivity in the sentence, whereas\n",
    "subjectivity implies whether sentence discusses something vaguely or with\n",
    "complete survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in c:\\users\\sai\\anaconda3\\lib\\site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\sai\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.1, subjectivity=1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "stmt = TextBlob (\"My home is far away from my school\")\n",
    "stmt.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How about you and I go together on a walk far away from this place,discussing the things we have never discussed on  Deep Learning and NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"How about you and I go together on a walk far away from this place,discussing the things we have never discussed on  Deep Learning and NLP\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_ = TextBlob(text)\n",
    "blob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How', 'WRB'),\n",
       " ('about', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('go', 'VBP'),\n",
       " ('together', 'RB'),\n",
       " ('on', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('walk', 'NN'),\n",
       " ('far', 'RB'),\n",
       " ('away', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('place', 'NN'),\n",
       " ('discussing', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('things', 'NNS'),\n",
       " ('we', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('never', 'RB'),\n",
       " ('discussed', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('Deep', 'NNP'),\n",
       " ('Learning', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('NLP', 'NNP')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TextBlob can also deal with spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think model needs to be trained more!\n"
     ]
    }
   ],
   "source": [
    "sample= TextBlob(\"I thinkk model needs to be trained more!\")\n",
    "print(sample.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n"
     ]
    }
   ],
   "source": [
    "lang = TextBlob(u\"Voulez-vous apprendre le francais?\")\n",
    "print(lang.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Do you want to learn French?\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang.translate(from_lang='fr',to='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n"
     ]
    }
   ],
   "source": [
    "lang = TextBlob(\"wie heißen sie\")\n",
    "print(lang.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's your name\n"
     ]
    }
   ],
   "source": [
    "print(lang.translate(from_lang='de',to='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te\n"
     ]
    }
   ],
   "source": [
    "lang = TextBlob(\"ఎం చేస్తున్నావ్\")\n",
    "print(lang.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are you doing\n"
     ]
    }
   ],
   "source": [
    "print(lang.translate(from_lang='te',to='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
